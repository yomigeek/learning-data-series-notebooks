{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6b834f",
   "metadata": {},
   "source": [
    "## How to Remove Outliers for Machine Learning\n",
    "Sources:\n",
    "- https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/\n",
    "- https://www.kaggle.com/code/kevinarvai/outlier-detection-practice-uni-multivariate/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10b25bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:32:12.617970Z",
     "start_time": "2023-11-23T13:32:12.605394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=50.049 stdv=4.994\n"
     ]
    }
   ],
   "source": [
    "# generate gaussian - normal distribution - data\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# seed the random number generator\n",
    "seed(1)\n",
    "# generate univariate observations\n",
    "data = 5 * randn(10000) + 50\n",
    "# summarize\n",
    "print('mean=%.3f stdv=%.3f' % (mean(data), std(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "170b9fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T13:32:43.855022Z",
     "start_time": "2023-11-23T13:32:43.840842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.12172682 46.94121793 47.35914124 ... 44.92928092 49.68651887\n",
      " 42.81065054]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289b312",
   "metadata": {},
   "source": [
    "## Standard Deviation Method\n",
    "If we know that the distribution of values in the sample is Gaussian or Gaussian-like, we can use the standard deviation of the sample as a cut-off for identifying outliers.\n",
    "\n",
    "The Gaussian distribution has the property that the standard deviation from the mean can be used to reliably summarize the percentage of values in the sample.\n",
    "\n",
    "For example, within one standard deviation of the mean will cover 68% of the data.\n",
    "\n",
    "So, if the mean is 50 and the standard deviation is 5, as in the test dataset above, then all data in the sample between 45 and 55 will account for about 68% of the data sample. We can cover more of the data sample if we expand the range as follows:\n",
    "\n",
    "- 1 Standard Deviation from the Mean: 68%\n",
    "- 2 Standard Deviations from the Mean: 95%\n",
    "- 3 Standard Deviations from the Mean: 99.7%\n",
    "\n",
    "A value that falls outside of 3 standard deviations is part of the distribution, but it is an unlikely or rare event at approximately 1 in 370 samples.\n",
    "\n",
    "Three standard deviations from the mean is a common cut-off in practice for identifying outliers in a Gaussian or Gaussian-like distribution. For smaller samples of data, perhaps a value of 2 standard deviations (95%) can be used, and for larger samples, perhaps a value of 4 standard deviations (99.9%) can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c965a4c",
   "metadata": {},
   "source": [
    "### Explaining the Standard Deviation from Mean:\n",
    "Sure, let's say you have a set of exam scores for a large group of students, and the scores follow a normal distribution. The mean score is 75, and the standard deviation is 10.\n",
    "\n",
    "68% within one standard deviation:\n",
    "\n",
    "Scores within one standard deviation of the mean (75 ± 10) would be between 65 and 85.\n",
    "Approximately 68% of the students scored between 65 and 85.\n",
    "95% within two standard deviations:\n",
    "\n",
    "Scores within two standard deviations of the mean (75 ± 2 * 10) would be between 55 and 95.\n",
    "Approximately 95% of the students scored between 55 and 95.\n",
    "99.7% within three standard deviations:\n",
    "\n",
    "Scores within three standard deviations of the mean (75 ± 3 * 10) would be between 45 and 105.\n",
    "Almost 99.7% of the students scored between 45 and 105.\n",
    "\n",
    "These calculations are based on the assumption that the distribution of scores is approximately normal. The empirical rule helps you understand how spread out the scores are and what proportion of students fall within certain ranges. Keep in mind that these are general rules and might not perfectly apply to every distribution, especially if the distribution deviates significantly from normality.\n",
    "\n",
    "#### If the distribution deviates significantly from normality, \n",
    "The empirical rule (68-95-99.7 rule) may not accurately describe the distribution, and relying on it might lead to incorrect interpretations. In such cases, it's important to consider the characteristics of the specific distribution and use alternative methods to analyze and describe the data.\n",
    "\n",
    "Here are a few steps you might consider:\n",
    "\n",
    "- **Visual Inspection**:\n",
    "\n",
    "Plot the data using a histogram or other visualization techniques to get a sense of its shape. Non-normal distributions may exhibit asymmetry, multiple peaks, or other patterns that deviate from the bell curve.\n",
    "\n",
    "- **Descriptive Statistics**:\n",
    "\n",
    "Calculate alternative measures of central tendency and dispersion. Median and interquartile range (IQR) are less sensitive to extreme values than the mean and standard deviation, respectively.\n",
    "Use Non-parametric Statistics:\n",
    "\n",
    "- **Non-parametric statistical tests do not assume a specific distribution**: \n",
    "\n",
    "Tests like the Mann-Whitney U test or Kruskal-Wallis test may be more appropriate for non-normally distributed data. \n",
    "\n",
    "- **Transformations**: \n",
    "\n",
    "Data transformations, such as logarithmic or square root transformations, might help make the distribution more normal. However, the appropriateness of transformations depends on the characteristics of the data.\n",
    "\n",
    "\n",
    "- **Bootstrapping**:\n",
    "\n",
    "Bootstrapping is a resampling technique that can provide robust estimates of confidence intervals, even when the distribution is unknown or deviates from normality.\n",
    "\n",
    "\n",
    "- **Consult Subject Matter Experts**:\n",
    "\n",
    "In some cases, it's essential to consult with subject matter experts who understand the context of the data. They may provide insights into whether the observed distribution makes sense in the given domain.\n",
    "Remember, the best approach depends on the specific characteristics of your data and the goals of your analysis. It's crucial to choose methods that are appropriate for the nature of the data and the research question at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5d1ec",
   "metadata": {},
   "source": [
    "#### We can calculate the mean and standard deviation of a given sample, then calculate the cut-off for identifying outliers as more than 3 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84340c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:11:07.654457Z",
     "start_time": "2023-11-23T15:11:07.604828Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate summary statistics\n",
    "data_mean, data_std = mean(data), std(data)\n",
    "# identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8979df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:12:29.252045Z",
     "start_time": "2023-11-23T15:12:29.241203Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can then identify outliers as those examples that fall outside of the defined lower and upper limits.\n",
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37060660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:19:23.069380Z",
     "start_time": "2023-11-23T15:19:23.038891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified outliers: 29\n",
      "Non-outlier observations: 9971\n"
     ]
    }
   ],
   "source": [
    "print('Identified outliers: %d' % len(outliers))\n",
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "print('Non-outlier observations: %d' % len(outliers_removed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ab3801",
   "metadata": {},
   "source": [
    "## Interquartile Range Method\n",
    "Not all data is normal or normal enough to treat it as being drawn from a Gaussian distribution.\n",
    "\n",
    "A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short.\n",
    "\n",
    "The IQR is calculated as the difference between the 75th and the 25th percentiles of the data and defines the box in a box and whisker plot.\n",
    "\n",
    "Remember that percentiles can be calculated by sorting the observations and selecting values at specific indices. The 50th percentile is the middle value, or the average of the two middle values for an even number of examples. If we had 10,000 samples, then the 50th percentile would be the average of the 5000th and 5001st values.\n",
    "\n",
    "We refer to the percentiles as quartiles (“quart” meaning 4) because the data is divided into four groups via the 25th, 50th and 75th values.\n",
    "\n",
    "The IQR defines the middle 50% of the data, or the body of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0197df",
   "metadata": {},
   "source": [
    "The interquartile range (IQR) is a measure of the spread or variability of a set of data. It is defined as the difference between the third quartile (Q3) and the first quartile (Q1) of the data, that is, IQR = Q3 - Q1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e2b445",
   "metadata": {},
   "source": [
    "The IQR can be used to identify outliers by defining limits on the sample values that are a factor k of the IQR below the 25th percentile or above the 75th percentile. The common value for the factor k is the value 1.5. A factor k of 3 or more can be used to identify values that are extreme outliers or “far outs” when described in the context of box and whisker plots.\n",
    "\n",
    "On a box and whisker plot, these limits are drawn as fences on the whiskers (or the lines) that are drawn from the box. Values that fall outside of these values are drawn as dots.\n",
    "\n",
    "We can calculate the percentiles of a dataset using the percentile() NumPy function that takes the dataset and specification of the desired percentile. The IQR can then be calculated as the difference between the 75th and 25th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf182d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:42:24.837917Z",
     "start_time": "2023-11-23T15:42:24.799095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles: 25th=46.685, 75th=53.359, IQR=6.674\n"
     ]
    }
   ],
   "source": [
    "# calculate interquartile range\n",
    "from numpy import percentile\n",
    "\n",
    "q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "iqr = q75 - q25\n",
    "print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c9e93f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:29:37.307339Z",
     "start_time": "2023-11-23T15:29:37.289784Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the outlier cutoff\n",
    "cut_off = iqr * 1.5\n",
    "lower, upper = q25 - cut_off, q75 + cut_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa649fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:29:46.704206Z",
     "start_time": "2023-11-23T15:29:46.694679Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "outliers = [x for x in data if x < lower or x > upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52ba5d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T15:30:01.113143Z",
     "start_time": "2023-11-23T15:30:01.102223Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "outliers_removed = [x for x in data if x > lower and x < upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e20efb",
   "metadata": {},
   "source": [
    "## Automatic Outlier Detection\n",
    "In machine learning, an approach to tackling the problem of outlier detection is one-class classification.\n",
    "\n",
    "One-Class Classification, or OCC for short, involves fitting a model on the “normal” data and predicting whether new data is normal or an outlier/anomaly.\n",
    "\n",
    "A one-class classifier is fit on a training dataset that only has examples from the normal class. Once prepared, the model is used to classify new examples as either normal or not-normal, i.e. outliers or anomalies.\n",
    "\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space.\n",
    "\n",
    "This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality.\n",
    "\n",
    "The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers.\n",
    "\n",
    "<i>We introduce a local outlier (LOF) for each object in the dataset, indicating its degree of outlier-ness.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba50f1",
   "metadata": {},
   "source": [
    "We can demonstrate the **LocalOutlierFactor** method on a predictive modelling dataset.\n",
    "\n",
    "We will use the Boston housing regression problem that has 13 inputs and one numerical target and requires learning the relationship between suburb characteristics and house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa5e7f8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:02:50.221753Z",
     "start_time": "2023-11-23T16:02:50.198961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "# load and summarize the dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "# load the dataset\n",
    "url = '../datasets/boston-housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# summarize the shape of the dataset\n",
    "print(X.shape, y.shape)\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the train and test sets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1c2e1",
   "metadata": {},
   "source": [
    "It is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric.\n",
    "\n",
    "In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE).\n",
    "\n",
    "The complete example of evaluating a linear regression model on the dataset is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19724af6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:21:46.815037Z",
     "start_time": "2023-11-23T16:21:46.610287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ff30d",
   "metadata": {},
   "source": [
    "\"MAE\" stands for Mean Absolute Error. Mean Absolute Error is a metric used to evaluate the performance of a regression model by measuring the average absolute differences between the actual and predicted values.\n",
    "\n",
    "Here's a breakdown of the statement \"the model achieved a MAE of about 3.417\":\n",
    "\n",
    "Mean Absolute Error (MAE): This is a measure of the average absolute differences between the actual values and the predicted values produced by a regression model. It's calculated by taking the average of the absolute differences between each actual and predicted value.\n",
    "\n",
    "\"of about 3.417\": The specific value of 3.417 is the computed Mean Absolute Error for the model on a given dataset. It indicates, on average, how far off the model's predictions are from the actual values. In this context, a MAE of 3.417 suggests that, on average, the model's predictions have an absolute error of approximately 3.417 units (whatever unit is used for the target variable).\n",
    "\n",
    "For example, if the target variable represents house prices in thousands of dollars, a MAE of 3.417 would mean that, on average, the model's predictions are off by about $3,417.\n",
    "\n",
    "In summary, a lower MAE indicates better performance, as it means that the model's predictions are, on average, closer to the actual values. The specific value of 3.417 provides a quantitative measure of the model's accuracy on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848330c",
   "metadata": {},
   "source": [
    "Next, we can try removing outliers from the training dataset.\n",
    "\n",
    "The expectation is that the outliers are causing the linear regression model to learn a bias or skewed understanding of the problem, and that removing these outliers from the training set will allow a more effective model to be learned.\n",
    "\n",
    "We can achieve this by defining the LocalOutlierFactor model and using it to make a prediction on the training dataset, marking each row in the training dataset as normal (1) or an outlier (-1). We will use the default hyperparameters for the outlier detection model, although it is a good idea to tune the configuration to the specifics of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd10912a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:26:17.105823Z",
     "start_time": "2023-11-23T16:26:16.984487Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78a725d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:26:47.527673Z",
     "start_time": "2023-11-23T16:26:47.506916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(305, 13) (305,)\n",
      "MAE: 3.356\n"
     ]
    }
   ],
   "source": [
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7730422",
   "metadata": {},
   "source": [
    "Running the example fits and evaluates the linear regression model with outliers deleted from the training dataset.\n",
    "\n",
    "Firstly, we can see that the number of examples in the training dataset has been reduced from 339 to 305, meaning 34 rows containing outliers were identified and deleted.\n",
    "\n",
    "We can also see a reduction in MAE from about 3.417 by a model fit on the entire training dataset, to about 3.356 on a model fit on the dataset with outliers removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82263ebe",
   "metadata": {},
   "source": [
    "## 4 Automatic Outlier Detection Algorithms in Python\n",
    "The scikit-learn library provides a number of built-in automatic methods for identifying outliers in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839bd72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:44:51.528595Z",
     "start_time": "2023-11-23T18:44:51.508069Z"
    }
   },
   "source": [
    "First we can calculate the Baseline model performance;\n",
    "\n",
    "### Baseline Model Performance\n",
    "It is a regression predictive modeling problem, meaning that we will be predicting a numeric value. All input variables are also numeric.\n",
    "\n",
    "In this case, we will fit a linear regression algorithm and evaluate model performance by training the model on the test dataset and making a prediction on the test data and evaluate the predictions using the mean absolute error (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5e29b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:52:55.197637Z",
     "start_time": "2023-11-23T18:52:54.873088Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.417\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on the raw dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c3a74",
   "metadata": {},
   "source": [
    "**In this case, we can see that the model achieved a MAE of about 3.417. This provides a baseline in performance to which we can compare different outlier identification and removal procedures.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839c330",
   "metadata": {},
   "source": [
    "### Isolation Forest\n",
    "Isolation Forest, or iForest for short, is a tree-based anomaly detection algorithm.\n",
    "\n",
    "It is based on modeling the normal data in such a way as to isolate anomalies that are both few in number and different in the feature space.\n",
    "\n",
    "<i>our proposed method takes advantage of two anomalies’ quantitative properties: i) they are the minority consisting of fewer instances and ii) they have attribute-values that are very different from those of normal instances.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a76a7e83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:19:50.528993Z",
     "start_time": "2023-11-23T18:19:50.294785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(305, 13) (305,)\n",
      "MAE: 3.178\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using isolation forest\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = '../datasets/boston-housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886f708",
   "metadata": {},
   "source": [
    "In the above case, we can see that that model identified and removed 34 outliers and achieved a MAE of about 3.178, an improvement over the baseline that achieved a score of about 3.417.\n",
    "\n",
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7be7e6f",
   "metadata": {},
   "source": [
    "### Minimum Covariance Determinant\n",
    "If the input variables have a Gaussian distribution, then simple statistical methods can be used to detect outliers.\n",
    "\n",
    "For example, if the dataset has two input variables and both are Gaussian, then the feature space forms a multi-dimensional Gaussian and knowledge of this distribution can be used to identify values far from the distribution.\n",
    "\n",
    "This approach can be generalized by defining a hypersphere (ellipsoid) that covers the normal data, and data that falls outside this shape is considered an outlier. An efficient implementation of this technique for multivariate data is known as the Minimum Covariance Determinant, or MCD for short.\n",
    "\n",
    "<i>The Minimum Covariance Determinant (MCD) method is a highly robust estimator of multivariate location and scatter, for which a fast algorithm is available. […] It also serves as a convenient and efficient tool for outlier detection.</i>|\n",
    "\n",
    "It provides the “contamination” argument that defines the expected ratio of outliers to be observed in practice. In this case, we will set it to a value of 0.01, found with a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55d0baae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:20:14.873005Z",
     "start_time": "2023-11-23T18:20:14.821954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(335, 13) (335,)\n",
      "MAE: 3.388\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using elliptical envelope\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = '../datasets/boston-housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8c676",
   "metadata": {},
   "source": [
    "In this case above, we can see that the elliptical envelope method identified and removed only 4 outliers, resulting in a drop in MAE from 3.417 with the baseline to 3.388."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b545f4a",
   "metadata": {},
   "source": [
    "### Local Outlier Factor\n",
    "A simple approach to identifying outliers is to locate those examples that are far from the other examples in the feature space.\n",
    "\n",
    "This can work well for feature spaces with low dimensionality (few features), although it can become less reliable as the number of features is increased, referred to as the curse of dimensionality.\n",
    "\n",
    "The local outlier factor, or LOF for short, is a technique that attempts to harness the idea of nearest neighbors for outlier detection. Each example is assigned a scoring of how isolated or how likely it is to be outliers based on the size of its local neighborhood. Those examples with the largest score are more likely to be outliers.\n",
    "\n",
    "The model provides the “contamination” argument, that is the expected percentage of outliers in the dataset, be indicated and defaults to 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8d364ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:31:28.066817Z",
     "start_time": "2023-11-23T18:31:28.032141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(305, 13) (305,)\n",
      "MAE: 3.356\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using local outlier factor\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = '../datasets/boston-housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d93bd7",
   "metadata": {},
   "source": [
    "In this case above, we can see that the local outlier factor method identified and removed 34 outliers, the same number as isolation forest, resulting in a drop in MAE from 3.417 with the baseline to 3.356. Better, but not as good as isolation forest, suggesting a different set of outliers were identified and removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0300072",
   "metadata": {},
   "source": [
    "### One-Class SVM\n",
    "The support vector machine, or SVM, algorithm developed initially for binary classification can be used for one-class classification.\n",
    "\n",
    "When modeling one class, the algorithm captures the density of the majority class and classifies examples on the extremes of the density function as outliers. This modification of SVM is referred to as One-Class SVM.\n",
    "\n",
    "<i>an algorithm that computes a binary function that is supposed to capture regions in input space where the probability density lives (its support), that is, a function such that most of the data will live in the region where the function is nonzero.</i>\n",
    "\n",
    "Although SVM is a classification algorithm and One-Class SVM is also a classification algorithm, it can be used to discover outliers in input data for both regression and classification datasets.\n",
    "\n",
    "The scikit-learn library provides an implementation of one-class SVM in the OneClassSVM class.\n",
    "\n",
    "The class provides the “nu” argument that specifies the approximate ratio of outliers in the dataset, which defaults to 0.1. In this case, we will set it to 0.01, found with a little trial and error.\n",
    "\n",
    "nu=0.01: The nu parameter is a hyperparameter in the One-Class SVM model that controls the trade-off between having a smooth decision boundary and classifying more or fewer data points as outliers. It represents an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. In this case, nu=0.01 means that the model is allowed to classify up to 1% of the training instances as outliers. Lower values of nu generally result in a stricter model that classifies fewer instances as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12b61baf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T18:35:16.457904Z",
     "start_time": "2023-11-23T18:35:16.407607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (339,)\n",
      "(336, 13) (336,)\n",
      "MAE: 3.431\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance with outliers removed using one class SVM\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load the dataset\n",
    "url = '../datasets/boston-housing.csv'\n",
    "df = read_csv(url, header=None)\n",
    "# retrieve the array\n",
    "data = df.values\n",
    "# split into input and output elements\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# identify outliers in the training dataset\n",
    "ee = OneClassSVM(nu=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "# fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# evaluate the model\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf2bf2",
   "metadata": {},
   "source": [
    "In this case, we can see that only three outliers were identified and removed and the model achieved a MAE of about 3.431, which is not better than the baseline model that achieved 3.417. Perhaps better performance can be achieved with more tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdfb750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

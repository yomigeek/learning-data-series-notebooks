{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebad3e8c",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2fd8d",
   "metadata": {},
   "source": [
    "## What is feature engineering?\n",
    "All machine learning algorithms use some input data to generate outputs. Input data contains many features which may not be in proper form to be given to the model directly. It needs some kind of processing and here feature engineering helps. Feature engineering fulfils mainly two goals:\n",
    "\n",
    "It prepares the input dataset in the form which is required for a specific model or machine learning algorithm.\n",
    "Feature engineering helps in improving the performance of machine learning models magically.\n",
    "\n",
    "The main feature engineering techniques that will be discussed are:\n",
    "\n",
    "1. Missing data imputation\n",
    "\n",
    "2. Categorical encoding\n",
    "\n",
    "3. Variable transformation\n",
    "\n",
    "4. Outlier engineering\n",
    "\n",
    "5.  Date and time engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5e9f7",
   "metadata": {},
   "source": [
    "### Missing Data Imputation for Feature Engineering\n",
    "Imputation is the act of replacing missing data with statistical estimates of the missing values. It helps you to complete your training data which can then be provided to any model or an algorithm for prediction.\n",
    "\n",
    "There are multiple techniques for missing data imputation. These are as follows:-\n",
    "\n",
    "1. Complete case analysis\n",
    "2. Mean / Median / Mode imputation\n",
    "3. Missing Value Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cabb7b4",
   "metadata": {},
   "source": [
    "#### Complete Case Analysis for Missing Data Imputation\n",
    "Complete case analysis is basically analyzing those observations in the dataset that contains values in all the variables. Or you can say, remove all the observations that contain missing values. But this method can only be used when there are only a few observations which has a missing dataset otherwise it will reduce the dataset size and then it will be of not much use.\n",
    "\n",
    "So, it can be used when missing data is small but in real-life datasets, the amount of missing data is always big. So, practically, complete case analysis is never an option to use, although you can use it if the missing data size is small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080a73f",
   "metadata": {},
   "source": [
    "#### Mean/ Median/ Mode for Missing Data Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060aad2",
   "metadata": {},
   "source": [
    "Missing values can also be replaced with the mean, median, or mode of the variable(feature). It is widely used in data competitions and in almost every situation. It is suitable to use this technique where data is missing at random places and in small proportions.\n",
    "\n",
    "\n",
    "impute missing values in age in train and test set\n",
    "\n",
    "median = X_train.Age.median()<br />\n",
    "for df in [X_train, X_test]:\n",
    "    df['Age'].fillna(median, inplace=True)<br />\n",
    "    \n",
    "X_train['Age'].isnull().sum()\n",
    "\n",
    "One important point to consider while doing imputation is that it should be done over the training set first and then to the test set. All missing values in the train set and test set should be filled with the value which is extracted from the train set only. This helps in avoiding overfitting.\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- **Imputing Missing Values**:\n",
    "\n",
    "The goal is to handle missing values in the 'Age' column of both the training set (X_train) and the test set (X_test).\n",
    "\n",
    "- **Choosing Imputation Value**:\n",
    "\n",
    "The median value of the 'Age' column in the training set (X_train) is calculated and stored in the variable median.\n",
    "\n",
    "- **Iterating Through DataFrames**:\n",
    "\n",
    "A loop is used to iterate over both the training set (X_train) and the test set (X_test).\n",
    "\n",
    "- **Filling Missing Values**:\n",
    "\n",
    "For each DataFrame (df), the missing values in the 'Age' column are filled with the previously calculated median value using the fillna method. This operation is done in place (inplace=True), meaning the original DataFrames are modified.\n",
    "\n",
    "- **Checking Missing Values After Imputation**:\n",
    "\n",
    "X_train['Age'].isnull().sum() is used to check the number of missing values in the 'Age' column of the training set after the imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcf08b",
   "metadata": {},
   "source": [
    "#### Missing Value Indicator For Missing Value Indication\n",
    "This technique involves adding a binary variable to indicate whether the value is missing for a certain observation. This variable takes the value 1 if the observation is missing, or 0 otherwise. But we still need to replace the missing values in the original variable, which we tend to do with mean or median imputation. By using these 2 techniques together, if the missing value has predictive power, it will be captured by the missing indicator, and if it doesn’t it will be masked by the mean / median imputation.\n",
    "\n",
    "X_train['Age_NA'] = np.where(X_train['Age'].isnull(), 1, 0)\n",
    "<br />\n",
    "X_test['Age_NA'] = np.where(X_test['Age'].isnull(), 1, 0)\n",
    "<br />\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b3d9b",
   "metadata": {},
   "source": [
    "X_train.Age.mean(), X_train.Age.median() = (29.915338645418327, 29.0)<br />\n",
    "Now, since mean and median are the same, let’s replace them with the median.\n",
    "\n",
    "X_train['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "<br />\n",
    "X_test['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "\n",
    "X_train.head(10)\n",
    "So, the Age_NA variable was created to capture the missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64309d1",
   "metadata": {},
   "source": [
    "The provided code and explanation describe a technique called \"Missing Value Indicator for Missing Value Indication.\" This technique involves creating a binary indicator variable to explicitly capture whether a particular observation has a missing value. Additionally, it includes imputing the missing values in the original variable using mean or median imputation.\n",
    "\n",
    "Let's break down the steps:\n",
    "\n",
    "1. **Create Missing Indicator Variable:**\n",
    "   ```python\n",
    "   X_train['Age_NA'] = np.where(X_train['Age'].isnull(), 1, 0)\n",
    "   X_test['Age_NA'] = np.where(X_test['Age'].isnull(), 1, 0)\n",
    "   ```\n",
    "   - This code creates a new binary variable, 'Age_NA,' which takes the value 1 if the corresponding 'Age' value is missing and 0 otherwise. This variable serves as an indicator of missingness.\n",
    "\n",
    "2. **Impute Missing Values with Median:**\n",
    "   ```python\n",
    "   X_train['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "   X_test['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "   ```\n",
    "   - The missing values in the 'Age' variable are imputed using the median of the non-missing values in the training set. Both the training and test sets are imputed with the median from the training set.\n",
    "\n",
    "3. **Reasoning:**\n",
    "   - The combination of creating a missing indicator and imputing with the median allows for capturing the information about missing values. If the fact that a value is missing has predictive power, it can be captured by the 'Age_NA' variable. Meanwhile, if the missing value itself is not informative, it gets replaced by the median value.\n",
    "\n",
    "4. **Check the Result:**\n",
    "   ```python\n",
    "   X_train.head(10)\n",
    "   ```\n",
    "   - This line prints the first 10 rows of the modified training set to show the impact of the missing value indicator and imputation.\n",
    "\n",
    "5. **Replace with Median when Mean and Median are the Same:**\n",
    "   ```python\n",
    "   X_train['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "   X_test['Age'].fillna(X_train.Age.median(), inplace=True)\n",
    "   ```\n",
    "   - In this specific case where the mean and median of the 'Age' variable are the same, the missing values are replaced with the median.\n",
    "\n",
    "The key idea is to retain information about missing values using the binary indicator while imputing missing values with a central tendency measure (median in this case). This approach is particularly useful when missingness itself might be predictive or informative in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a2d60",
   "metadata": {},
   "source": [
    "### Categorical encoding in Feature Engineering\n",
    "Categorical data is defined as that data that takes only a number of values. Let’s understand this with an example. Parameter Gender in a dataset will have categorical values like Male, Female. If a survey is done to know which car people own then the result will be categorical (because the answers would be in categories like Honda, Toyota, Hyundai, Maruti, None, etc.). So, the point to notice here is that data falls in a fixed set of categories.\n",
    "\n",
    "If you directly give this dataset with categorical variables to a model, you will get an error. Hence, they are required to be encoded. There are multiple techniques to do so:\n",
    "\n",
    "1. One-Hot encoding (OHE)\n",
    "2. Ordinal encoding\n",
    "3. Count and Frequency encoding\n",
    "4. Target encoding / Mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd262e",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c5585",
   "metadata": {},
   "source": [
    "It is a commonly used technique for encoding categorical variables. It basically creates binary variables for each category present in the categorical variable. These binary variables will have 0 if it is absent in the category or 1 if it is present. Each new variable is called a dummy variable or binary variable.\n",
    "\n",
    "Example: using this color approach below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3340b251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T17:15:50.035843Z",
     "start_time": "2023-11-24T17:15:49.977735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   ID  Color\n",
      "0   1    Red\n",
      "1   2  Green\n",
      "2   3   Blue\n",
      "3   4    Red\n",
      "4   5  Green\n",
      "\n",
      "One-Hot Encoding without dropping any variable:\n",
      "    Blue  Green    Red\n",
      "0  False  False   True\n",
      "1  False   True  False\n",
      "2   True  False  False\n",
      "3  False  False   True\n",
      "4  False   True  False\n",
      "\n",
      "Concatenated Dataset:\n",
      "   Color   Blue  Green    Red\n",
      "0    Red  False  False   True\n",
      "1  Green  False   True  False\n",
      "2   Blue   True  False  False\n",
      "3    Red  False  False   True\n",
      "4  Green  False   True  False\n",
      "\n",
      "One-Hot Encoding with drop_first=True:\n",
      "   Green    Red\n",
      "0  False   True\n",
      "1   True  False\n",
      "2  False  False\n",
      "3  False   True\n",
      "4   True  False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dummy dataset with a categorical variable 'Color'\n",
    "data = {'ID': [1, 2, 3, 4, 5],\n",
    "        'Color': ['Red', 'Green', 'Blue', 'Red', 'Green']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Perform one-hot encoding without dropping any variable\n",
    "encoded_df = pd.get_dummies(df['Color'])\n",
    "print(\"\\nOne-Hot Encoding without dropping any variable:\")\n",
    "print(encoded_df)\n",
    "\n",
    "# Concatenate the original 'Color' column with dummy variables\n",
    "concatenated_df = pd.concat([df['Color'], encoded_df], axis=1)\n",
    "print(\"\\nConcatenated Dataset:\")\n",
    "print(concatenated_df)\n",
    "\n",
    "# Perform one-hot encoding with drop_first=True\n",
    "encoded_df_drop_first = pd.get_dummies(df['Color'], drop_first=True)\n",
    "print(\"\\nOne-Hot Encoding with drop_first=True:\")\n",
    "print(encoded_df_drop_first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9eaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T17:16:26.762473Z",
     "start_time": "2023-11-24T17:16:26.752714Z"
    }
   },
   "source": [
    "**Original Dataset**:\n",
    "\n",
    "    ID   Color\n",
    "    1    Red\n",
    "    2    Green\n",
    "    3    Blue\n",
    "    4    Red\n",
    "    5    Green\n",
    "\n",
    "\n",
    "\n",
    "**One-Hot Encoding without dropping any variable**:\n",
    "\n",
    "          Blue  Green  Red\n",
    "    0     0      0    1\n",
    "    1     0      1    0\n",
    "    2     1      0    0\n",
    "    3     0      0    1\n",
    "    4     0      1    0\n",
    "\n",
    "\n",
    "**Concatenated Dataset**:\n",
    "\n",
    "             Color  Blue  Green  Red\n",
    "        0    Red     0      0    1\n",
    "        1  Green     0      1    0\n",
    "        2   Blue     1      0    0\n",
    "        3    Red     0      0    1\n",
    "        4  Green     0      1    0\n",
    "\n",
    "\n",
    "**One-Hot Encoding with drop_first=True**:\n",
    "\n",
    "        Green  Red\n",
    "        0      0    1\n",
    "        1      1    0\n",
    "        2      0    0\n",
    "        3      0    1\n",
    "        4      1    0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9950fdd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T17:18:25.168451Z",
     "start_time": "2023-11-24T17:18:25.159672Z"
    }
   },
   "source": [
    "The last output shows one-hot encoding with the drop_first=True argument, resulting in n-1 dummy variables. In this case, 'Blue' is dropped, and 'Green' and 'Red' are represented by a single dummy variable each.\n",
    "\n",
    "When using one-hot encoding with drop_first=True, one of the categorical levels is dropped to avoid multicollinearity in certain statistical models, such as linear regression. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, making it difficult to determine the individual effect of each variable on the response variable.\n",
    "\n",
    "In the context of one-hot encoding:\n",
    "\n",
    "If you have a categorical variable with n levels, creating n dummy variables would introduce perfect multicollinearity because knowing the values of n-1 dummy variables would uniquely determine the value of the remaining dummy variable.\n",
    "\n",
    "By dropping one of the dummy variables, you prevent multicollinearity issues. The dropped variable becomes the reference category, and the information about that category is captured by the other dummy variables.\n",
    "\n",
    "In the context of one-hot encoding with drop_first=True:\n",
    "<br />\n",
    "\n",
    "- If 'Green' is 0 and 'Red' is 0, it implies that both 'Green' and 'Red' are 0, which further implies that the dropped category ('Blue' in this case) is 1.\n",
    "\n",
    "- If both 'Green' and 'Red' are 0, it means 'Blue' is 1.\n",
    "\n",
    "The logic is derived from the fact that only one of the dummy variables should be 1 at a time, and the dropped category can be inferred by the absence of the other dummy variables. Let me break down the reasoning:\n",
    "\n",
    "- Scenario 1 ('Green' is 0 and 'Red' is 0):\n",
    "\n",
    "    'Green' is 0, indicating that the category 'Green' is not present.\n",
    "    'Red' is 0, indicating that the category 'Red' is not present.\n",
    "     Since both 'Green' and 'Red' are 0, it implies that the dropped category (in this case, 'Blue') is 1.\n",
    "     \n",
    "- Scenario 2 (both 'Green' and 'Red' are 0):\n",
    "\n",
    "     Both 'Green' and 'Red' are 0, indicating that neither 'Green' nor 'Red' is present.\n",
    "     Since both are 0, it implies that the dropped category ('Blue') is 1.\n",
    "\n",
    "In summary, the values of the dummy variables are such that if the dropped category is not present (both 'Green' and 'Red' are 0), then the dropped category is indicated by the value 1 in the dropped variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfd53f",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "Ordinal encoding is a method of encoding categorical variables where the categories have a meaningful order or ranking. This is commonly used when dealing with variables that have an inherent order or hierarchy among them. For example, grades in an exam (A, B, C, D, Fail) or education levels (High School, Bachelor's, Master's, Ph.D.) are ordinal variables.\n",
    "\n",
    "Here's an example of how ordinal encoding can be done using Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425262e0",
   "metadata": {},
   "source": [
    "#### Using Manual Mapping with a Dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde9dcd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T19:14:08.940643Z",
     "start_time": "2023-11-24T19:14:08.828567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Student Grade  Grade_Ordinal\n",
      "0    Alice     A              4\n",
      "1      Bob     B              3\n",
      "2  Charlie     C              2\n",
      "3    David     D              1\n",
      "4      Eve  Fail              0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data with a categorical variable 'Grade'\n",
    "data = {'Student': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "        'Grade': ['A', 'B', 'C', 'D', 'Fail']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mapping ordinal labels to numerical values\n",
    "grade_mapping = {'Fail': 0, 'D': 1, 'C': 2, 'B': 3, 'A': 4}\n",
    "\n",
    "# Applying ordinal encoding to the 'Grade' column\n",
    "df['Grade_Ordinal'] = df['Grade'].map(grade_mapping)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f55ca15",
   "metadata": {},
   "source": [
    "In this example above:\n",
    "\n",
    "The original DataFrame has two columns, 'Student' and 'Grade.'\n",
    "\n",
    "A dictionary (grade_mapping) is created to map ordinal labels to numerical values. In this case, 'Fail' is assigned 0, 'D' is assigned 1, and so on.\n",
    "\n",
    "The map function is used to apply the ordinal encoding to the 'Grade' column, creating a new column called 'Grade_Ordinal.'\n",
    "\n",
    "The resulting DataFrame would look like this:\n",
    "\n",
    "       Student Grade  Grade_Ordinal\n",
    "    0    Alice     A              4\n",
    "    1      Bob     B              3\n",
    "    2  Charlie     C              2\n",
    "    3    David     D              1\n",
    "    4      Eve  Fail              0\n",
    "    \n",
    "    \n",
    "Now, the 'Grade_Ordinal' column represents the ordinal encoding of the original 'Grade' column, where higher values indicate higher grades. This encoding retains the ordinal relationship among the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f57335",
   "metadata": {},
   "source": [
    "#### Using LabelEncoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a554adaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T19:30:33.438318Z",
     "start_time": "2023-11-24T19:30:33.426477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: ['Medium', 'Small', 'Large', 'Medium', 'Large', 'Small']\n",
      "Encoded Sizes: [1 2 0 1 0 2]\n",
      "Decoded Sizes: ['Medium' 'Small' 'Large' 'Medium' 'Large' 'Small']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Sample data with a categorical variable 'Size'\n",
    "sizes = ['Medium', 'Small', 'Large', 'Medium', 'Large', 'Small']\n",
    "\n",
    "# Create a LabelEncoder\n",
    "size_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder on the list of categories\n",
    "size_encoder.fit(sizes)\n",
    "\n",
    "# Transform labels to encoded values\n",
    "encoded_sizes = size_encoder.transform(sizes)\n",
    "\n",
    "# Display the results\n",
    "print(\"Original Sizes:\", sizes)\n",
    "print(\"Encoded Sizes:\", encoded_sizes)\n",
    "\n",
    "# Inverse transform to get original labels back\n",
    "decoded_sizes = size_encoder.inverse_transform(encoded_sizes)\n",
    "print(\"Decoded Sizes:\", decoded_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07fe6d",
   "metadata": {},
   "source": [
    "**Pros and Cons**:\n",
    "\n",
    "- LabelEncoder:\n",
    "\n",
    "    Pros:\n",
    "    - Compact and convenient for simple cases.\n",
    "    - No need to create a separate mapping dictionary manually.\n",
    "\n",
    "    Cons:\n",
    "    - May not be as transparent if you want to inspect or modify the mapping.\n",
    "    - Less control over the encoding if you need a custom mapping.\n",
    "    <br />\n",
    "    <br />\n",
    "    \n",
    "- Manual Mapping with a Dictionary:\n",
    "\n",
    "    Pros:\n",
    "    - Explicit control over the mapping.\n",
    "    - Clear visibility of the mapping.\n",
    "    <br />\n",
    "    \n",
    "    Cons:\n",
    "    - Requires manual creation and maintenance of the mapping dictionary.\n",
    "    - More verbose for large datasets or many categories.\n",
    "\n",
    "In practice, the choice between these methods depends on the complexity of your encoding needs and personal coding preferences. The ``LabelEncoder`` is convenient and concise, while manual mapping provides more explicit control. For simple ordinal encoding tasks, either method is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9396cd",
   "metadata": {},
   "source": [
    "### Count and Frequency Encoding\n",
    "\n",
    "#### Count Encoding\n",
    "In count encoding, each category is replaced with the count of occurrences of that category in the dataset. This can be a useful encoding technique when the frequency or prevalence of each category is important information for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c94ac712",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T20:11:24.765914Z",
     "start_time": "2023-11-24T20:11:24.683218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Color  Color_Count\n",
      "0   1    Red            3\n",
      "1   2   Blue            2\n",
      "2   3  Green            2\n",
      "3   4    Red            3\n",
      "4   5  Green            2\n",
      "5   6   Blue            2\n",
      "6   7    Red            3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data with a categorical variable 'Color'\n",
    "data = {'ID': [1, 2, 3, 4, 5, 6, 7],\n",
    "        'Color': ['Red', 'Blue', 'Green', 'Red', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform count encoding\n",
    "count_encoding = df['Color'].value_counts().to_dict()\n",
    "\n",
    "# Map the counts back to the original DataFrame\n",
    "df['Color_Count'] = df['Color'].map(count_encoding)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0afc2",
   "metadata": {},
   "source": [
    "#### Frequency Encoding:\n",
    "\n",
    "Frequency encoding is similar to count encoding, but instead of using the count, it uses the frequency or proportion of each category in the dataset. This can be particularly useful when the relative prevalence of categories is more important than their absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50d78828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T20:12:26.734307Z",
     "start_time": "2023-11-24T20:12:26.688931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Color  Color_Frequency\n",
      "0   1    Red         0.428571\n",
      "1   2   Blue         0.285714\n",
      "2   3  Green         0.285714\n",
      "3   4    Red         0.428571\n",
      "4   5  Green         0.285714\n",
      "5   6   Blue         0.285714\n",
      "6   7    Red         0.428571\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data with a categorical variable 'Color'\n",
    "data = {'ID': [1, 2, 3, 4, 5, 6, 7],\n",
    "        'Color': ['Red', 'Blue', 'Green', 'Red', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform frequency encoding\n",
    "frequency_encoding = (df['Color'].value_counts() / len(df)).to_dict()\n",
    "\n",
    "# Map the frequencies back to the original DataFrame\n",
    "df['Color_Frequency'] = df['Color'].map(frequency_encoding)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcad37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
